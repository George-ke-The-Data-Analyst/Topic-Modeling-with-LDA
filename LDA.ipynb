{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ba27a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: science, data, learning, powerful, makes\n",
      "Topic 1: fun, data, science, requires, learning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"Data science is fun\",\n",
    "    \"Machine learning makes data science powerful\",\n",
    "    \"Science requires data and learning.\"\n",
    "]\n",
    "\n",
    "# 1. Vectorize\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# 2. Fit LDA\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# 3. Display top words per topic\n",
    "#    Use get_feature_names() on older sklearn versions\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [feature_names[i] for i in topic.argsort()[:-6:-1]]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_terms)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f92984",
   "metadata": {},
   "source": [
    "This code demonstrates how to use Latent Dirichlet Allocation (LDA), a topic modeling technique, to extract topics from a small set of text documents. Here's a breakdown of the code:\n",
    "\n",
    "### 1. **Importing Libraries**\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "- `LatentDirichletAllocation`: A class from `scikit-learn` used for topic modeling.\n",
    "- `CountVectorizer`: Converts text data into a bag-of-words representation (a matrix of token counts).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Defining the Documents**\n",
    "```python\n",
    "docs = [\n",
    "    \"Data science is fun\",\n",
    "    \"Machine learning makes data science powerful\",\n",
    "    \"Science requires data and learning.\"\n",
    "]\n",
    "```\n",
    "- A small list of text documents is defined. These will be used as input for topic modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Vectorizing the Text**\n",
    "```python\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(docs)\n",
    "```\n",
    "- `CountVectorizer` is initialized with `stop_words='english'` to remove common English stop words (e.g., \"is\", \"and\").\n",
    "- `fit_transform(docs)` converts the text into a sparse matrix (`X`) where rows represent documents and columns represent word counts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Fitting the LDA Model**\n",
    "```python\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "```\n",
    "- `LatentDirichletAllocation` is initialized with `n_components=2`, meaning it will extract 2 topics.\n",
    "- `fit(X)` trains the LDA model on the bag-of-words matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Extracting and Displaying Topics**\n",
    "```python\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [feature_names[i] for i in topic.argsort()[:-6:-1]]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_terms)}\")\n",
    "```\n",
    "- `get_feature_names()` retrieves the list of words (features) from the `CountVectorizer`.\n",
    "- `lda.components_` contains the word distributions for each topic. Each topic is represented as a vector of word probabilities.\n",
    "- `topic.argsort()[:-6:-1]` sorts the words by their importance to the topic and selects the top 5 words.\n",
    "- The top words for each topic are printed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "The output will display the top 5 words for each of the 2 topics. For example:\n",
    "```\n",
    "Topic 0: science, data, learning, machine, requires\n",
    "Topic 1: data, science, fun, powerful, learning\n",
    "```\n",
    "\n",
    "This helps identify the main themes or topics in the given documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2898312",
   "metadata": {},
   "source": [
    "This code demonstrates how to use Latent Dirichlet Allocation (LDA), a topic modeling technique, to extract topics from a small set of text documents. Here's a breakdown of the code:\n",
    "\n",
    "### 1. **Importing Libraries**\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "- `LatentDirichletAllocation`: A class from `scikit-learn` used for topic modeling.\n",
    "- `CountVectorizer`: Converts text data into a bag-of-words representation (a matrix of token counts).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Defining the Documents**\n",
    "```python\n",
    "docs = [\n",
    "    \"Data science is fun\",\n",
    "    \"Machine learning makes data science powerful\",\n",
    "    \"Science requires data and learning.\"\n",
    "]\n",
    "```\n",
    "- A small list of text documents is defined. These will be used as input for topic modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Vectorizing the Text**\n",
    "```python\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(docs)\n",
    "```\n",
    "- `CountVectorizer` is initialized with `stop_words='english'` to remove common English stop words (e.g., \"is\", \"and\").\n",
    "- `fit_transform(docs)` converts the text into a sparse matrix (`X`) where rows represent documents and columns represent word counts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Fitting the LDA Model**\n",
    "```python\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "```\n",
    "- `LatentDirichletAllocation` is initialized with `n_components=2`, meaning it will extract 2 topics.\n",
    "- `fit(X)` trains the LDA model on the bag-of-words matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Extracting and Displaying Topics**\n",
    "```python\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [feature_names[i] for i in topic.argsort()[:-6:-1]]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_terms)}\")\n",
    "```\n",
    "- `get_feature_names()` retrieves the list of words (features) from the `CountVectorizer`.\n",
    "- `lda.components_` contains the word distributions for each topic. Each topic is represented as a vector of word probabilities.\n",
    "- `topic.argsort()[:-6:-1]` sorts the words by their importance to the topic and selects the top 5 words.\n",
    "- The top words for each topic are printed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "The output will display the top 5 words for each of the 2 topics. For example:\n",
    "```\n",
    "Topic 0: science, data, learning, machine, requires\n",
    "Topic 1: data, science, fun, powerful, learning\n",
    "```\n",
    "\n",
    "This helps identify the main themes or topics in the given documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
